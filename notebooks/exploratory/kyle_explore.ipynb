{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lime\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/ee/4aaac4cd79f16329746495aca96f8c35f278b5c774eff3358eaa21e1cbf3/lime-0.2.0.0.tar.gz (274kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from lime) (3.1.1)\n",
      "Requirement already satisfied: numpy in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from lime) (1.16.5)\n",
      "Requirement already satisfied: scipy in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from lime) (1.4.1)\n",
      "Requirement already satisfied: tqdm in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from lime) (4.36.1)\n",
      "Collecting pillow==5.4.1 (from lime)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/96/05a5c9ba4a75ed330234780e4ae30018bbf77c847982ff6a16b8b291a0da/Pillow-5.4.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 37.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.18 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from lime) (0.22.1)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from lime) (0.16.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib->lime) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib->lime) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib->lime) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib->lime) (2.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-learn>=0.18->lime) (0.13.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (2.8.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image>=0.12->lime) (2.3)\n",
      "Requirement already satisfied: six in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->lime) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->lime) (41.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.0)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lime: filename=lime-0.2.0.0-cp36-none-any.whl size=284181 sha256=f1907c1a9c6388ad9f32069e5906c5aacf7fe7f90c0cb79eea1557729dec6959\n",
      "  Stored in directory: /Users/kyledecember1/Library/Caches/pip/wheels/22/f2/ec/e5ebd07348b2b1ac722e91c2f549fcc220f7d5f25497a61232\n",
      "Successfully built lime\n",
      "Installing collected packages: pillow, lime\n",
      "  Found existing installation: Pillow 6.1.0\n",
      "    Uninstalling Pillow-6.1.0:\n",
      "      Successfully uninstalled Pillow-6.1.0\n",
      "Successfully installed lime-0.2.0.0 pillow-5.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "                rescale=1./255)\n",
    "\n",
    "train_path = os.path.join(os.pardir, os.pardir, 'data/train')\n",
    "final_test_path = os.path.join(os.pardir, os.pardir, 'data/val')\n",
    "test_path = os.path.join(os.pardir, os.pardir, 'data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test known model to ensure functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (image_size, image_size, 3)\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  keras.layers.GlobalAveragePooling2D(),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = test_generator.n // batch_size\n",
    "\n",
    "history = model1.fit_generator(train_generator,\n",
    "                              steps_per_epoch = steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              workers=4,\n",
    "                              validation_data=test_generator,\n",
    "                              validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Made Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some convolutional layers, along with basic dropout for regularization \n",
    "\n",
    "model2 = tf.keras.Sequential()\n",
    "\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Dropout(.5))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Dropout(.3))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit_generator(train_generator,\n",
    "                     steps_per_epoch = steps_per_epoch,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator,\n",
    "                     validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing images to 160x160\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some convolutional layers, along with basic dropout for regularization, \n",
    "\n",
    "model3 = tf.keras.Sequential()\n",
    "\n",
    "model3.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Dropout(.5))\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "model3.add(layers.Dropout(.3))\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model3.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model3.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis: variant accuracy indicated dropout may be too high early in network, unclear if issue is overfit or dropout coincidence\n",
    "\n",
    "solve: remove dropout entirely, then add backwards (from output layer to input layer), until stable and accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some convolutional layers, along with basic dropout for regularization, \n",
    "\n",
    "model4 = tf.keras.Sequential()\n",
    "\n",
    "model4.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model4.add(layers.Flatten())\n",
    "model4.add(layers.Dense(64, activation='relu'))\n",
    "model4.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model4.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model4.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  increasing batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size64 = 64\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size64,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size64,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = tf.keras.Sequential()\n",
    "\n",
    "model5.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Flatten())\n",
    "model5.add(layers.Dense(64, activation='relu'))\n",
    "model5.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size64\n",
    "validation_steps160 = test_generator160.n // batch_size64\n",
    "\n",
    "model5.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model5.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysis:  batch size 64 performed slightly better, but lack of regularization hurt the overall performance\n",
    "\n",
    "solve: add batch normalization to each layer, and view results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization: batch normalization w/ batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size64 = 64\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size64,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size64,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = tf.keras.Sequential()\n",
    "\n",
    "model6.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(layers.BatchNormalization())\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(layers.BatchNormalization())\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Flatten())\n",
    "model6.add(layers.Dense(64, activation='relu'))\n",
    "model6.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size64\n",
    "validation_steps160 = test_generator160.n // batch_size64\n",
    "\n",
    "model6.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model6.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularization: batch normalization w/ batch size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0414 14:26:52.197391 4722408896 deprecation.py:506] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model7 = tf.keras.Sequential()\n",
    "\n",
    "model7.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model7.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model7.add(layers.BatchNormalization())\n",
    "model7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model7.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model7.add(layers.BatchNormalization())\n",
    "model7.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model7.add(layers.Flatten())\n",
    "model7.add(layers.Dense(64, activation='relu'))\n",
    "model7.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0414 14:26:53.204540 4722408896 deprecation.py:323] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 266s 2s/step - loss: 0.2270 - acc: 0.9304 - val_loss: 0.9777 - val_acc: 0.6217\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 229s 1s/step - loss: 0.0756 - acc: 0.9751 - val_loss: 1.2767 - val_acc: 0.6234\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 226s 1s/step - loss: 0.0453 - acc: 0.9837 - val_loss: 1.1286 - val_acc: 0.6546\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 233s 1s/step - loss: 0.0283 - acc: 0.9898 - val_loss: 1.1076 - val_acc: 0.7664\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 207s 1s/step - loss: 0.0156 - acc: 0.9937 - val_loss: 3.9594 - val_acc: 0.6826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13a9c75f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model7.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model7.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding dropout layers / increasing total layers of convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = tf.keras.Sequential()\n",
    "\n",
    "model8.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model8.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model8.add(layers.BatchNormalization())\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model8.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model8.add(layers.BatchNormalization())\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model8.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model8.add(layers.BatchNormalization())\n",
    "model8.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model8.add(layers.Flatten())\n",
    "model8.add(layers.Dense(256, activation='relu'))\n",
    "model8.add(layers.Dropout(.3))\n",
    "model8.add(layers.Dense(128, activation='relu'))\n",
    "model8.add(layers.Dropout(.2))\n",
    "model8.add(layers.Dense(64, activation='relu'))\n",
    "model8.add(layers.Dropout(.1))\n",
    "\n",
    "model8.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 281s 2s/step - loss: 0.2128 - acc: 0.9231 - val_loss: 2.2559 - val_acc: 0.6266\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 273s 2s/step - loss: 0.1299 - acc: 0.9555 - val_loss: 4.1423 - val_acc: 0.6266\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 281s 2s/step - loss: 0.0967 - acc: 0.9703 - val_loss: 2.2651 - val_acc: 0.6414\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 317s 2s/step - loss: 0.0745 - acc: 0.9757 - val_loss: 0.3036 - val_acc: 0.8914\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 270s 2s/step - loss: 0.0586 - acc: 0.9835 - val_loss: 5.3896 - val_acc: 0.6447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13aff86d8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model8.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model8.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis\n",
    "\n",
    "every model beyond model 3 has only decreased validation accuracy\n",
    "\n",
    "removing batch normalization from model 8 to investigate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = tf.keras.Sequential()\n",
    "\n",
    "model9.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model9.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model9.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model9.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model9.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model9.add(layers.Flatten())\n",
    "model9.add(layers.Dense(256, activation='relu'))\n",
    "model9.add(layers.Dropout(.3))\n",
    "model9.add(layers.Dense(128, activation='relu'))\n",
    "model9.add(layers.Dropout(.2))\n",
    "model9.add(layers.Dense(64, activation='relu'))\n",
    "model9.add(layers.Dropout(.1))\n",
    "\n",
    "model9.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 201s 1s/step - loss: 0.4682 - acc: 0.7960 - val_loss: 0.3398 - val_acc: 0.8651\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 199s 1s/step - loss: 0.1990 - acc: 0.9224 - val_loss: 0.6866 - val_acc: 0.7763\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 223s 1s/step - loss: 0.1469 - acc: 0.9461 - val_loss: 0.6164 - val_acc: 0.7763\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 207s 1s/step - loss: 0.1094 - acc: 0.9601 - val_loss: 0.6047 - val_acc: 0.8224\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 208s 1s/step - loss: 0.0979 - acc: 0.9628 - val_loss: 0.8417 - val_acc: 0.7862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13c743748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model9.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model9.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis\n",
    "\n",
    "improved performance without Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adding more dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = tf.keras.Sequential()\n",
    "\n",
    "model10.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model10.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model10.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model10.add(layers.MaxPooling2D((2, 2)))\n",
    "model10.add(layers.Dropout(.2))\n",
    "\n",
    "model10.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model10.add(layers.MaxPooling2D((2, 2)))\n",
    "model10.add(layers.Dropout(.2))\n",
    "\n",
    "model10.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model10.add(layers.MaxPooling2D((2, 2)))\n",
    "model10.add(layers.Dropout(.2))\n",
    "\n",
    "model10.add(layers.Flatten())\n",
    "model10.add(layers.Dense(256, activation='relu'))\n",
    "model10.add(layers.Dropout(.3))\n",
    "model10.add(layers.Dense(128, activation='relu'))\n",
    "model10.add(layers.Dropout(.2))\n",
    "model10.add(layers.Dense(64, activation='relu'))\n",
    "model10.add(layers.Dropout(.1))\n",
    "\n",
    "model10.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "163/163 [==============================] - 288s 2s/step - loss: 0.5253 - acc: 0.7584 - val_loss: 0.4900 - val_acc: 0.8421\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 229s 1s/step - loss: 0.2897 - acc: 0.8806 - val_loss: 0.3709 - val_acc: 0.8553\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 227s 1s/step - loss: 0.2027 - acc: 0.9179 - val_loss: 0.5105 - val_acc: 0.7763\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 233s 1s/step - loss: 0.1551 - acc: 0.9406 - val_loss: 0.3815 - val_acc: 0.8372\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 225s 1s/step - loss: 0.1346 - acc: 0.9469 - val_loss: 0.4697 - val_acc: 0.7829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x140c10780>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model10.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model10.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analysis\n",
    "\n",
    "still overfitting to training data, increasing dropout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4986 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary',\n",
    "                shuffle=False)\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary',\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0416 10:24:30.513206 4538322368 deprecation.py:506] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0416 10:24:30.674054 4538322368 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0416 10:24:30.730453 4538322368 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model11 = tf.keras.Sequential()\n",
    "\n",
    "model11.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model11.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "model11.add(layers.Dropout(.3))\n",
    "\n",
    "model11.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "model11.add(layers.Dropout(.4))\n",
    "\n",
    "model11.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "model11.add(layers.Dropout(.6))\n",
    "\n",
    "model11.add(layers.Flatten())\n",
    "model11.add(layers.Dense(256, activation='relu'))\n",
    "model11.add(layers.Dropout(.6))\n",
    "model11.add(layers.Dense(128, activation='relu'))\n",
    "model11.add(layers.Dropout(.4))\n",
    "model11.add(layers.Dense(64, activation='relu'))\n",
    "model11.add(layers.Dropout(.2))\n",
    "\n",
    "model11.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  9/155 [>.............................] - ETA: 7:02 - loss: 0.2952 - acc: 0.9271"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-deadadbaa682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                      \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                      \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator160\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                      validation_steps=validation_steps160)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    674\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager_dataset_or_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# Make sure that y, sample_weights, validation_split are not passed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model11.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model11.fit(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import metrics\n",
    "\n",
    "import os,sys\n",
    "try:\n",
    "    import lime\n",
    "except:\n",
    "    sys.path.append(os.path.join('..', '..')) # add the current directory\n",
    "    import lime\n",
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0416 10:46:12.340668 4538322368 deprecation.py:506] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0416 10:46:12.341748 4538322368 deprecation.py:506] From /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0416 10:46:12.489216 4538322368 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0416 10:46:12.543236 4538322368 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model11.save('nn_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model_new = load_model('nn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[214,  20],\n",
       "       [ 70, 320]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for x in model_new.predict_generator(test_generator160):\n",
    "    for z in x:\n",
    "        preds.append(np.round(z))\n",
    "        \n",
    "metrics.confusion_matrix(test_generator160.labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 246 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "final_test_generator = datagen.flow_from_directory(\n",
    "                final_test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary',\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[75, 55],\n",
       "       [81, 35]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = []\n",
    "for x in model_new.predict_generator(final_test_generator):\n",
    "    for z in x:\n",
    "        predictions2.append(np.round(z))\n",
    "        \n",
    "metrics.confusion_matrix(final_test_generator.labels, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/6b/f889a95b0eea50b4648c7cf782e339f7f8e46e89023f14b3c685cf86b99f/scikit_image-0.16.2-cp36-cp36m-macosx_10_6_intel.whl (30.4MB)\n",
      "\u001b[K     |████████████████████████████████| 30.4MB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image) (3.1.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image) (6.1.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from scikit-image) (2.3)\n",
      "Collecting PyWavelets>=0.4.0 (from scikit-image)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/cd/0d96e765d793ae0e2fa291250ab98c27c0c574b0044c5a6ec3f6ae2afa91/PyWavelets-1.1.1-cp36-cp36m-macosx_10_9_x86_64.whl (4.3MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3MB 29.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio>=2.3.0 (from scikit-image)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/2b/9dd19644f871b10f7e32eb2dbd6b45149c350b4d5f2893e091b882e03ab7/imageio-2.8.0-py3-none-any.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 39.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.16.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from networkx>=2.0->scikit-image) (4.4.0)\n",
      "Requirement already satisfied: six in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/kyledecember1/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (41.2.0)\n",
      "Installing collected packages: PyWavelets, imageio, scikit-image\n",
      "Successfully installed PyWavelets-1.1.1 imageio-2.8.0 scikit-image-0.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-cbe41089664e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexplanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator160\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhide_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/lime/lime_image.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mexplanations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgray2rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "explanation = explainer.explain_instance(train_generator160[0], model_new.predict, top_labels=5, hide_color=0, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13224185],\n",
       "       [0.15948376],\n",
       "       [0.22123545],\n",
       "       [0.17403579],\n",
       "       [0.15958577],\n",
       "       [0.17377862],\n",
       "       [0.20354733],\n",
       "       [0.17014503],\n",
       "       [0.17615831],\n",
       "       [0.15804839],\n",
       "       [0.19419083],\n",
       "       [0.19881377],\n",
       "       [0.18041053],\n",
       "       [0.18751505],\n",
       "       [0.20761913],\n",
       "       [0.22741958],\n",
       "       [0.12730455],\n",
       "       [0.16603217],\n",
       "       [0.20559838],\n",
       "       [0.12726372],\n",
       "       [0.16384733],\n",
       "       [0.22310248],\n",
       "       [0.18498936],\n",
       "       [0.18415824],\n",
       "       [0.14804041],\n",
       "       [0.1996808 ],\n",
       "       [0.17181745],\n",
       "       [0.14631048],\n",
       "       [0.14830452],\n",
       "       [0.24849802],\n",
       "       [0.22717094],\n",
       "       [0.20597667]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = 'person27_bacteria_135.jpeg'\n",
    "img = image.load_img(img_path, target_size=(160, 160))\n",
    "# x = image.img_to_array(img)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis\n",
    "\n",
    "performed better with higher levels of dropout\n",
    "\n",
    "next steps:  \n",
    "shift heavier weight dropout to end of network   \n",
    "remove dropout from Convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4986 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = 160\n",
    "batch_size = 32\n",
    "\n",
    "train_generator160 = datagen.flow_from_directory(\n",
    "                train_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')\n",
    "\n",
    "test_generator160 = datagen.flow_from_directory(\n",
    "                test_path,  # Source directory for the training images\n",
    "                target_size=(image_size, image_size),\n",
    "                batch_size=batch_size,\n",
    "                # Since we use binary_crossentropy loss, we need binary labels\n",
    "                class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0415 13:54:01.555767 4652023232 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model11 = tf.keras.Sequential()\n",
    "\n",
    "model11.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model11.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model11.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model11.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model11.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model11.add(layers.Flatten())\n",
    "model11.add(layers.Dense(256, activation='relu'))\n",
    "model11.add(layers.Dropout(.6))\n",
    "model11.add(layers.Dense(128, activation='relu'))\n",
    "model11.add(layers.Dropout(.4))\n",
    "model11.add(layers.Dense(64, activation='relu'))\n",
    "model11.add(layers.Dropout(.2))\n",
    "\n",
    "model11.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "155/155 [==============================] - 199s 1s/step - loss: 0.5482 - acc: 0.7539 - val_loss: 0.7950 - val_acc: 0.6217\n",
      "Epoch 2/5\n",
      "155/155 [==============================] - 184s 1s/step - loss: 0.3401 - acc: 0.8171 - val_loss: 0.6482 - val_acc: 0.7418\n",
      "Epoch 3/5\n",
      "155/155 [==============================] - 197s 1s/step - loss: 0.2194 - acc: 0.9160 - val_loss: 0.4331 - val_acc: 0.8487\n",
      "Epoch 4/5\n",
      "155/155 [==============================] - 199s 1s/step - loss: 0.1689 - acc: 0.9396 - val_loss: 0.8239 - val_acc: 0.7549\n",
      "Epoch 5/5\n",
      "155/155 [==============================] - 199s 1s/step - loss: 0.1390 - acc: 0.9516 - val_loss: 1.0411 - val_acc: 0.7451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13ba44748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model11.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model11.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis\n",
    "\n",
    "too overfit\n",
    "increase dropout @ the dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0415 14:24:04.050050 4652023232 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "model12 = tf.keras.Sequential()\n",
    "\n",
    "model12.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model12.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model12.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model12.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model12.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model12.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model12.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model12.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model12.add(layers.Flatten())\n",
    "model12.add(layers.Dense(256, activation='relu'))\n",
    "model12.add(layers.Dropout(.8))\n",
    "model12.add(layers.Dense(128, activation='relu'))\n",
    "model12.add(layers.Dropout(.6))\n",
    "model12.add(layers.Dense(64, activation='relu'))\n",
    "model12.add(layers.Dropout(.4))\n",
    "\n",
    "model12.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "155/155 [==============================] - 227s 1s/step - loss: 0.5930 - acc: 0.7350 - val_loss: 0.6278 - val_acc: 0.6217\n",
      "Epoch 2/5\n",
      "155/155 [==============================] - 235s 2s/step - loss: 0.4410 - acc: 0.7551 - val_loss: 0.4977 - val_acc: 0.6217\n",
      "Epoch 3/5\n",
      "155/155 [==============================] - 203s 1s/step - loss: 0.3135 - acc: 0.7719 - val_loss: 0.6032 - val_acc: 0.6382\n",
      "Epoch 4/5\n",
      "155/155 [==============================] - 182s 1s/step - loss: 0.2683 - acc: 0.8833 - val_loss: 0.5134 - val_acc: 0.7089\n",
      "Epoch 5/5\n",
      "155/155 [==============================] - 182s 1s/step - loss: 0.2503 - acc: 0.9158 - val_loss: 0.7992 - val_acc: 0.6793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13e4c0748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model12.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model12.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add batchnormalization to convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model13 = tf.keras.Sequential()\n",
    "\n",
    "model13.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model13.add(layers.BatchNormalization())\n",
    "model13.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model13.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model13.add(layers.BatchNormalization())\n",
    "model13.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model13.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model13.add(layers.BatchNormalization())\n",
    "model13.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model13.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model13.add(layers.BatchNormalization())\n",
    "model13.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model13.add(layers.Flatten())\n",
    "model12.add(layers.Dense(256, activation='relu'))\n",
    "model13.add(layers.Dropout(.8))\n",
    "model13.add(layers.Dense(128, activation='relu'))\n",
    "model13.add(layers.Dropout(.6))\n",
    "model13.add(layers.Dense(64, activation='relu'))\n",
    "model13.add(layers.Dropout(.4))\n",
    "\n",
    "model13.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "155/155 [==============================] - 342s 2s/step - loss: 0.8122 - acc: 0.7951 - val_loss: 4.3175 - val_acc: 0.6217\n",
      "Epoch 2/5\n",
      "155/155 [==============================] - 338s 2s/step - loss: 0.4050 - acc: 0.8864 - val_loss: 8.2578 - val_acc: 0.6217\n",
      "Epoch 3/5\n",
      "155/155 [==============================] - 376s 2s/step - loss: 0.3105 - acc: 0.9005 - val_loss: 6.9283 - val_acc: 0.6217\n",
      "Epoch 4/5\n",
      "155/155 [==============================] - 467s 3s/step - loss: 0.2820 - acc: 0.9162 - val_loss: 6.8764 - val_acc: 0.6234\n",
      "Epoch 5/5\n",
      "155/155 [==============================] - 364s 2s/step - loss: 0.2477 - acc: 0.9253 - val_loss: 4.2100 - val_acc: 0.6414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17c4d24e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model13.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model13.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reomove batch normalization\n",
    "## increase learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model14 = tf.keras.Sequential()\n",
    "\n",
    "model14.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model14.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model14.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model14.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model14.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model14.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model14.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model14.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model14.add(layers.Flatten())\n",
    "model14.add(layers.Dense(256, activation='relu'))\n",
    "model14.add(layers.Dropout(.8))\n",
    "model14.add(layers.Dense(128, activation='relu'))\n",
    "model14.add(layers.Dropout(.6))\n",
    "model14.add(layers.Dense(64, activation='relu'))\n",
    "model14.add(layers.Dropout(.4))\n",
    "\n",
    "model14.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "155/155 [==============================] - 194s 1s/step - loss: 0.5430 - acc: 0.7453 - val_loss: 0.6052 - val_acc: 0.7582\n",
      "Epoch 2/5\n",
      "155/155 [==============================] - 193s 1s/step - loss: 0.3127 - acc: 0.8694 - val_loss: 0.6599 - val_acc: 0.8536\n",
      "Epoch 3/5\n",
      "155/155 [==============================] - 193s 1s/step - loss: 0.2047 - acc: 0.9279 - val_loss: 1.0452 - val_acc: 0.8043\n",
      "Epoch 4/5\n",
      "155/155 [==============================] - 202s 1s/step - loss: 0.1612 - acc: 0.9495 - val_loss: 1.7028 - val_acc: 0.7878\n",
      "Epoch 5/5\n",
      "155/155 [==============================] - 197s 1s/step - loss: 0.1445 - acc: 0.9554 - val_loss: 1.3855 - val_acc: 0.7434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17cc19898>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model14.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model14.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change optimizer to Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15 = tf.keras.Sequential()\n",
    "\n",
    "model15.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)))\n",
    "model15.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model15.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model15.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model15.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model15.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model15.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model15.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model15.add(layers.Flatten())\n",
    "model15.add(layers.Dense(256, activation='relu'))\n",
    "model15.add(layers.Dropout(.6))\n",
    "model15.add(layers.Dense(128, activation='relu'))\n",
    "model15.add(layers.Dropout(.3))\n",
    "model15.add(layers.Dense(64, activation='relu'))\n",
    "model15.add(layers.Dropout(.1))\n",
    "\n",
    "model15.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "155/155 [==============================] - 183s 1s/step - loss: 0.3946 - acc: 0.8325 - val_loss: 0.8341 - val_acc: 0.7730\n",
      "Epoch 2/10\n",
      "155/155 [==============================] - 178s 1s/step - loss: 0.1554 - acc: 0.9421 - val_loss: 0.7726 - val_acc: 0.8141\n",
      "Epoch 3/10\n",
      "155/155 [==============================] - 175s 1s/step - loss: 0.1178 - acc: 0.9588 - val_loss: 1.1350 - val_acc: 0.7648\n",
      "Epoch 4/10\n",
      "155/155 [==============================] - 176s 1s/step - loss: 0.0951 - acc: 0.9661 - val_loss: 1.4714 - val_acc: 0.7862\n",
      "Epoch 5/10\n",
      "155/155 [==============================] - 174s 1s/step - loss: 0.0852 - acc: 0.9734 - val_loss: 1.0854 - val_acc: 0.7566\n",
      "Epoch 6/10\n",
      "155/155 [==============================] - 177s 1s/step - loss: 0.0714 - acc: 0.9715 - val_loss: 1.0796 - val_acc: 0.7878\n",
      "Epoch 7/10\n",
      "155/155 [==============================] - 175s 1s/step - loss: 0.0557 - acc: 0.9804 - val_loss: 1.2744 - val_acc: 0.8141\n",
      "Epoch 8/10\n",
      "155/155 [==============================] - 175s 1s/step - loss: 0.0533 - acc: 0.9802 - val_loss: 1.2605 - val_acc: 0.8240\n",
      "Epoch 9/10\n",
      "155/155 [==============================] - 175s 1s/step - loss: 0.0470 - acc: 0.9816 - val_loss: 2.5145 - val_acc: 0.7188\n",
      "Epoch 10/10\n",
      "155/155 [==============================] - 176s 1s/step - loss: 0.0413 - acc: 0.9855 - val_loss: 1.5539 - val_acc: 0.7862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x181fbb320>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch160 = train_generator160.n // batch_size\n",
    "validation_steps160 = test_generator160.n // batch_size\n",
    "\n",
    "model15.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model15.fit_generator(train_generator160,\n",
    "                     steps_per_epoch = steps_per_epoch160,\n",
    "                     epochs=epochs,\n",
    "                     workers=4,\n",
    "                     validation_data=test_generator160,\n",
    "                     validation_steps=validation_steps160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
